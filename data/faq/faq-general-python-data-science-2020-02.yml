-
  Q: I feel overwhelmed with this dataset. what should I do?
  A: "Take it one column at a time. Identify your target variable. For example, you could select the home price in a table of real estate prices for homes. `y = df['price']`. Then look at the dtype (data type) for each of the columns and work with just the first numeric column you see, for example the total square footage of the home. `X = df[['sqft']]`. Then you can train a linear regression to predict home price from square footage. `lr, lr = sklearn.linear_model.LinearRegression(), lr.fit(X, y)` and you can score it for accuracy with `lr.score(X, y)`. Then add one column at a time to your variable X (features) and see how your `fit` improves by checking the `score` each time."
-
  Q: "How can I filter out proper nouns from supreme court rulings so I can build a model to predict the judge's name and use that to suggest words for lawyers to use in their arguments?"
  A: "Spacy has a built-in POS tagger: `[tok.text for tok in spacy.load('en')('Hello Earth!') if tok.pos_ != 'PROPN']` or:
  >>> import spacy
  >>> nlp = spacy.load('en_core_web_md')
  >>> doc = nlp('Hello world from Hobson Lane in Mississippi sitting on the john.')
  >>> [tok.pos_ for tok in doc]
  ['INTJ', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'ADP', 'PROPN', 'VERB', 'ADP', 'DET', 'PROPN', 'PUNCT']
  >>> [(tok.text, tok.pos_) for tok in doc]
  "
-
  Q: "When I bootstrapped a shuffled dataset and calculated the confidence 95% interval the mean of my actual data is well outside the 95% confidence interval. What does this mean?"
  A: "This means that the two means are statistically different and are unlikely to have occurred by chance so you can reject your null hypothesis, just as if you'd gotten a p-value that was smaller than 5% (or 1 - .95)."
-
  Q: "What is the bias-variance trade-off? What does the U shaped parabola mean when the horizontal axis is model complexity and the vertical axis is the model prediction error? #interview #teacher #student"
  A: "The bias is the mean absolute error (MAE) or mean absolute precision (MAP) on your test set. This is what you care about the most. The variance is the mean absolute error or mean absolute precision (MAP) on your training set. The bias-variance trade-off means that as you increase model complexity, while keeping the information content or variety of your features constant, both bias and variance will be reduced until eventually your your bias will start to increase, even though your variance is continuing to get smaller. Takeaway: when your test set error starts increasing, stop adding model complexity and start simplifying your model."
-
  Q: "In the bias-variance tradeoff for your model you want to reduce model variance (the error on your training set). How can you increase model complexity without adding new features (columns) to your dataset? #interview #teacher #student"
  A: "You can add addition transformations of your existing features. One easy transformation is to add an additional polynomial term, such as `x_1**2` or `x_1**3`, as a feature without adding any new feature with new information about the world. Another common way is to add a nonlinear term to your model, like the product or quotient of two features that are already included in your model, such as `x_1 * x_2` or `x_1 / x_2`. Transcendental functions are also good nonlinear transformations to try when you have  a feature variable that is proportional to time or or location (or any situation where you target variable or input feature variable might be periodic), such as `sin(x_1)` and `cos(x_1)`."
-
  Q: "The song classification dataset seems to difficult for me because I don't know how to load the audio files into python."
  A: "You can ignore the audio files and just train a model on the meta data about the songs."
-
  Q: "Should I try to find more data or generate and extract more features from my existing data?"
  A: "If you have geographic data like city, state, address, or even latiitude/longitude data, often the best features are hidden within your existing data. For example one very predictive feature for a crim dataset is the distance to the nearest liquor store or the distance to the nearest transit station or stop (subway or bus)."
-
  Q: "I don't understand how to interpret a scatter plot of predicted vs truth."
  A: "I find it better to plot the truth on the horizontal axis and the error or residual on the vertical axis. This is called the residual plot. It will help you identify any nonlinearity in your target variable's relationship to your features so that you can engineer new nonlinear features or perhaps use a nonlinear model to improve its accuracy."
-
  Q: "How can I deal with my shipping data that doesn't have any shipments from or too some zip codes."
  A: "Your model will have to generalize from one zipcode to another. That is accomplished by joining your zipcode data on another continuous variable related to that zipcode, like population, latitude and longitude, maximum speed limit, etc. Any demographic or statistical or geographic information that might be related to shipping cost can be added to your dataset to help your model generalize from one zip code to another."
-
  Q: "Would predicting basketball players success based on their stats in the first year be a good data science project? #student"
  A: "Probably not. It's better to start with a dataset that you can easily download and manipulate and let the data define the question or hypothesis you want to answer. If you try to dream up a problem that you will be interested in, it may not be solvable with publicly available data. But if you are able to find a column in a CSV file that looks like it would be interesting as a target variable (the variable you are trying to predict), they you know that the problem is solvable. You may not get the accuracy you'd like, but you at least know you'll be able to build a model to attempt to predict it."
-
  Q: "What kind of dataset would you need to try to predict whether a price is going to increase or decrease tomorrow?"
  A: "You would need a table of prices and other features along with a date. You would then need to shift the price data you want to predict by one day into the future. Your price for the current day (indicated by the date column) is a feature. You price for tomorrow (the next trading date after the date in the date column) would be your target variable."
-
  Q: "How should I deal with 250k vocabulary causing my computer to crash when I try to turn my TfidfVectorizer output into a dense `DataFrame`? #student #bigdata"
  A: "Work with a much smaller vocabulary by increasing your min_df to 3, 4 or 5 and decreasing your max_df to .5 or lower. You can temporarily build models that work with a subset of the data to do hyper parameter tuning. For a multiclass classifier this means stratifying your data into the target  classes and training on only a few of the classes at a time. Once you've found estimates of hyperparameters for these smaller models you can use batch training (.partial_fit() combined with a csv_reader) to minimize the amount of data you need to hold in RAM at any one time."
-
  Q: "What is an ROC curve and how does in apply to data science and machine learning?"
  A: "An ROC curve or Radio Operating Characteristic curve shows the change in precision and recall in a classification problem where you adjust the threshold you use for classification over the full range of possible thresholds. For a radio this is the precision and recall of a radio receiver at classifying an incoming blip as either a 1 or zero that was sent by the transmitter in a digital signal. If you increase you threshold you are going to miss more ones, reducing your recall. If you decrease your threshold you are going to miss fewer ones but misclassify more zeros as ones, reducing your precision but increasing your recall. So in data science we often plot the precision recall curve with precision on the horizontal axis and recall on the vertical axis. F1 score is just the area inside of a box with one corner on that curve for the particular threshold that your model chose. You don't have to retrain a model to change your F1 score, you can just reuse all your weights and parameters and just change the decision threshold at the last step of the algorithm. In logistic regression in sklearn you'd just use `.predict_proba()` to get a probability prediction between zero and one and classify things as a 1 if the probability exceeds .6 or .4 or whatever threshold you chose, rather than the .5 value that logistic regression uses by default."
-
  Q: "I'm on data cleaning data. How do I filter out values that are too big in one column of a DataFrame?"
  A: "Use a mask to detect the outliers. If the column is named 'height' and the maximum height you want to allow in your dataset is 8, you would create a mask or filter to select all these outliers with `mask = df['height'] > 8`. You could then change all these large values to be exactly 8 with `df['height'][mask] = 8`. #beginner #datascience #python #datacleaning"
-
  Q: "What is the difference between using Inferential statistics vs using bootstrapping to calculate a p-value or T-test."
  A: "For inferential statistics you rely on what you know about the distribution of your data based on the parameters you calculated from your data like the mean and the standard deviation. You can use the values to calculate the probability  that your data was created by random chance. For the bootstrapping approach you can draw random samples from your data and compute how often the values meet whatever criteria your hypothesis has proposed. This will tell you how often a random dataset would meet your criteria. This probability of your null hypothesis is then your estimate of the p-value for your hypothesis. #statistics #datascience"
-
  Q: "What is an AttributeError?"
  A: "You are trying to call a method or attribute on a object that doesn't exist in the class definition for that object. You can learn about classes, objects, methods, and attributes at [learnpython.org](http://www.learnpython.org/en/Classes_and_Objects)"
-
  Q: "Where can I learn about classes and objects and methods and attributes?"
  A: "You can learn about classes, objects, methods, and attributes at [learnpython.org](http://www.learnpython.org/en/Classes_and_Objects)"
-
  Q: "Is scaling data required as part of the ml pipeline? #student"
  A: "Not usually. Like most steps in a machine learning pipeline are optional and you get to decide based on your understanding and experience whether each step is useful or not in your particualr situation."
-
  Q: "Should I use TF-IDF vectors or bag-of-words vectors to predict author names if I want to be able to recommend vocabulary and style to imitate an author? #student #nlp"
  A: "A bag of words model with be more explainable to actionable. It will tell you the exact frequency of all the words in your vocabulary that would produce a good match to your desired author. However a TF-IDF vector model will be more accurate and recommend you use more impactful words more or less often, and thus may help you build models that sound like another author without using the exact same word frequencies."
-
  Q: "How can I use a model that predicts overdose rates by counties based on the demographic data in those counties to predict the portion of each demographic group (age, gender, ethnicity) will overdose in those counties or nationwide?"
  A: "Compose a fictional county with the demographics you are interested in (total population, all of the same gender, age range, ethnicity) and then predict the overdose rate in that county that is the overdose rate that is most likely to result within that ethnic group in that part of the country (if geographic data like state and latitude/longitude are also features in your model)."
-
  Q: "Why are there so many different measures of accuracy like RMSE and MSE and R-squared score? Which one is used most often in the real world? #student"
  A: "Each one has a different intuition behind it and a different use case. In the scientific community people tend to use RMSE (root mean squared error) because it represents a precise statistic called standard deviation or 1-sigma error or standard error. It has a very precise interpretation in statistics. For example it can be used in the the 68-95-99.7 rule for estimating confidence bounds for the error at plus or minus 1-sigma, 2-sigma, and 3-sigma. Some data scientists and engineers use MSE because it represents the error variance or residual variance and can be directly added to other measures of error to create the total error variance from multiple pieces of a system. Other people prefer the R-squared correlation or score because its scale is independent of the units of measure for your target variable. And R-squared score always ranges between zero and one. This makes it easier to compare one model to another. So this is often used in hyperparameter optimization to build Data Science intuition about the affect of hyperparameters on your error across different datasets and problems. Each error measure has a use. Chose the one you understand the best and can explain to your customers the easiest. #data-science #statistics #model-evaluation"
